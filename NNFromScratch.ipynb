{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmerDiaaeldin/testrepo/blob/main/NNFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vzUDhkkddAi"
      },
      "source": [
        "## Building a neural network from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2vweLucd76j"
      },
      "source": [
        "Trying to build the neural network using OOP <br>\n",
        "I'll define a layer class, then use it to define a neural network class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Cq62HIloD3dX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "RX2oIwkh4AmJ"
      },
      "outputs": [],
      "source": [
        "# Define a layer class, now this is a base class for generally any NN layer. All\n",
        "# layers have input, output, forward_prop, backward_prop. Now depending on the NN\n",
        "# one.\n",
        "# one can use this base class to add features specific to the application they have in\n",
        "# mind (Pooling, convolution, activation,...etc) \n",
        "\n",
        "class Layer:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  #returns the output of the layer given the input\n",
        "  def forward_propagation(self, input):\n",
        "    return NotImplementedError\n",
        "\n",
        "  #returns the dE/dX for a given dY/dX (and update parameters)\n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    return NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ka6cBFftdIud"
      },
      "outputs": [],
      "source": [
        "class FCLayer(Layer):\n",
        "\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.W = np.random.rand(output_size, input_size) - 0.5 #initialize W, the parameter matrix\n",
        "    self.b = np.random.rand(output_size, 1) - 0.5 #initialize b, the bias\n",
        "\n",
        "  #for the forward propagation I'm going to feed it W and the input, and return from it the output of the layer\n",
        "  def forward_propagation(self, input):\n",
        "    #print(f\"W's shape : {self.W.shape}\")\n",
        "    #print(f\"X's shape : {input.shape}\")\n",
        "    #print(f\"b's shape : {self.b.shape}\")\n",
        "    self.input = input  #assign the layer input when forward propagating\n",
        "    self.output = np.matmul(self.W, self.input) + self.b\n",
        "    #print(self.output[0])\n",
        "    return self.output\n",
        "\n",
        "  #computes dE/dW and dE/db for a given output_error = dE/dY, updates W and b, and finally returns dE/dX\n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    input_error = np.matmul(np.transpose(self.W),output_error)\n",
        "    W_error = np.matmul(output_error, np.transpose(self.input))\n",
        "    #b_error = output_error\n",
        "    #print(f\"output's error : {output_error.shape}\")\n",
        "    #print(f\"b's error : {b_error.shape}\")\n",
        "\n",
        "    #update the parameters\n",
        "    self.W -= learning_rate * W_error\n",
        "    self.b -= learning_rate * np.sum(output_error,axis=1,keepdims=True)\n",
        "\n",
        "    return input_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yi4um6QtNRP"
      },
      "source": [
        "Now let's code some non-linear layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "Wzamg7LftM7f"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ActivationLayer(Layer):\n",
        "\n",
        "  def __init__(self, activation, activation_derivative):\n",
        "    super().__init__()\n",
        "    self.activation = activation\n",
        "    self.activation_derivative = activation_derivative\n",
        "\n",
        "  def forward_propagation(self, input):\n",
        "    self.input = input\n",
        "    self.output = self.activation(self.input)\n",
        "    return self.output\n",
        "\n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    return self.activation_derivative(self.input)*output_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6ZQgUtjN4N7"
      },
      "source": [
        "tanh and relu activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "ljdkkz3_th-L"
      },
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "  return (1-np.tanh(x)**2)\n",
        "\n",
        "def relu(x):\n",
        "  return max(0,x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return (x>0)*1\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  s = sigmoid(x)\n",
        "  return s*(1-s)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbiEtmM3N8Tm"
      },
      "source": [
        "Time to code losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "zjvcdlumN-uY"
      },
      "outputs": [],
      "source": [
        "def mse(y_true, y_pred):\n",
        "  return np.mean(np.power(y_true-y_pred,2))\n",
        "\n",
        "def mse_derivative(y_true, y_pred):\n",
        "  return 2*(y_pred-y_true)/y_true.size\n",
        "\n",
        "def entropy(y_true, y_pred):\n",
        "  #print(\"y_pred min = \", np.min(y_pred))\n",
        "  lg1 = np.log(y_pred)\n",
        "  print(\"lg1 min\",np.min(lg1))\n",
        "  z1 = -1*np.multiply(y_true,lg1)\n",
        "  lg0 = np.log(1-y_pred)\n",
        "  print(\"lg0 min\",np.min(lg0))\n",
        "  z0 = -1*np.multiply(1-y_true,lg0)\n",
        "  z = np.mean(z0 + z1)\n",
        "  return np.mean(z)\n",
        "\n",
        "def entropy_derivative(y_true, y_pred):\n",
        "  result = -y_true*1/y_pred + (1-y_true)/(1-y_pred)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "\n",
        "  z = np.argmax(y_pred, axis = 0)\n",
        "  prediction = np.zeros(y_pred.shape)\n",
        "  for element in enumerate(z):\n",
        "    prediction[tuple(reversed(element))] = 1\n",
        "\n",
        "  return 100*(1 - (np.sum(np.abs(y_true-prediction))//2)/prediction.size)"
      ],
      "metadata": {
        "id": "X_irGFIeoyVN"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5heE3fKwv3cI"
      },
      "source": [
        "# Finally the network class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "HsC9gOEdv3HN"
      },
      "outputs": [],
      "source": [
        "class Network:  #it is basically a list of layer objects\n",
        "  def __init__(self):\n",
        "    self.loss = None\n",
        "    self.layers = []\n",
        "    self.loss_derivative = None\n",
        "\n",
        "  #adds a layer object to the network\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  #set the loss function of the network\n",
        "  def use(self, loss, loss_derivative):\n",
        "    self.loss = loss\n",
        "    self.loss_derivative = loss_derivative\n",
        "\n",
        "  #make predictions\n",
        "  def predict(self,input_data):\n",
        "    # the input data must be a rank-2 tensor (matrix) (# of examples x # of features)\n",
        "    output = input_data\n",
        "    #print(output.shape)\n",
        "    i = 1\n",
        "    for layer in self.layers:\n",
        "      #print(i)\n",
        "      i += 1\n",
        "      output = layer.forward_propagation(output)\n",
        "      #print(output.shape)\n",
        "    return output\n",
        "\n",
        "  #fit the network\n",
        "  def fit(self, X_train, y_train, epochs, learning_rate):\n",
        "    for epoch in range(epochs):\n",
        "      y_pred = self.predict(X_train)\n",
        "      #print(\"y_pred min : \", np.min(y_pred))\n",
        "      #print(\"y_pred max : \", np.max(y_pred))\n",
        "      err = self.loss(y_train, y_pred)\n",
        "      #print(\"losses = \", err)\n",
        "      error = self.loss_derivative(y_train, y_pred)\n",
        "      #print(\"minimum derivative = \", np.min(error))\n",
        "      #print(\"maximum derivative = \", np.max(error))\n",
        "      for layer in self.layers[-1::-1]:\n",
        "        error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "      err = np.mean(err)\n",
        "      print(f\"epoch {epoch+1}/{epochs} error = {err}\")\n",
        "      print(\"test accuracy : \",accuracy(y_train, y_pred),end=\"\\n\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYH1kz8o6Iw5"
      },
      "source": [
        "Testing time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjcWbupzcqSj"
      },
      "source": [
        "Trying the mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "EFYaMZX47_8h"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "data = mnist.load_data()\n",
        "(x_train,y_train), (x_test, y_test) = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLiclG3fd-jo"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape,\n",
        "y_train.shape\n",
        ",x_test.shape\n",
        ",y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0TJjy7edCcW"
      },
      "outputs": [],
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "print(y_train.shape)\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0],28*28).transpose()\n",
        "y_train = y_train.transpose()\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]).transpose()\n",
        "y_test = y_test.transpose()\n",
        "\n",
        "x_train = x_train.astype('float')\n",
        "x_train /= 255\n",
        "x_test = x_test.astype('float')\n",
        "x_test /= 255\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJW81f6EeUPe"
      },
      "outputs": [],
      "source": [
        "classifier = Network()\n",
        "\n",
        "classifier.add(FCLayer(28*28, 200))\n",
        "classifier.add(ActivationLayer(activation = tanh, activation_derivative = tanh_derivative))\n",
        "classifier.add(FCLayer(200, 100))\n",
        "classifier.add(ActivationLayer(activation = tanh, activation_derivative = tanh_derivative))\n",
        "classifier.add(FCLayer(100, 50))\n",
        "classifier.add(ActivationLayer(activation = tanh, activation_derivative = tanh_derivative))\n",
        "classifier.add(FCLayer(50, 10))\n",
        "classifier.add(ActivationLayer(activation = sigmoid, activation_derivative = sigmoid_derivative))\n",
        "\n",
        "classifier.use(mse, mse_derivative)\n",
        "classifier.fit(x_train[:][:1000],y_train[:][:1000],epochs = 100, learning_rate = 25)\n",
        "result = classifier.predict(x_train[:][:1000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = classifier.predict(x_test)\n",
        "print(f\"test set accuracy : {accuracy(y_test,test)}\")"
      ],
      "metadata": {
        "id": "PNr0TtDxHa7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def digit(array):\n",
        "  return np.argmax(array)"
      ],
      "metadata": {
        "id": "2377aB-MJOT3"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def show_image_and_pred(img_number):\n",
        "  z = x_train[:,img_number:img_number+1]\n",
        "  image = z.reshape(28,28)\n",
        "  result = classifier.predict(z)\n",
        "  fig = plt.figure\n",
        "  plt.imshow(image, cmap='gray')\n",
        "  plt.show\n",
        "  print(f\"prediction = {digit(result)}\")"
      ],
      "metadata": {
        "id": "iBVtHRGZcPFq"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "choice = randrange(0,10000)\n",
        "print(f\"digit number {choice+1} of the test set\",end = \"\\n\\n\")\n",
        "show_image_and_pred(choice)  "
      ],
      "metadata": {
        "id": "ISdVVv_1IaIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t4J1tSBiO3gL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyOfyQIYW7toe8uB1c9wGqzd",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}